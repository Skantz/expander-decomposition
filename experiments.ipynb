{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#placeholder\n",
    "data = [0.6, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metis_partition_file_converter(inp, out):\n",
    "    import sys\n",
    "\n",
    "    #inp = sys.argv[1]\n",
    "    #out = sys.argv[2]\n",
    "\n",
    "    partitions = {}\n",
    "\n",
    "    def add(el, idx):\n",
    "        if idx not in partitions:\n",
    "            partitions[idx] = [el]\n",
    "        else:\n",
    "            partitions[idx].append(el)\n",
    "\n",
    "\n",
    "    with open(inp, \"r+\") as f:\n",
    "        el = 0\n",
    "        for line in f:\n",
    "            idx = int(line.strip())\n",
    "            add(el, idx)\n",
    "            el += 1\n",
    "\n",
    "    with open(out, \"w+\") as f:\n",
    "        idxs = list(partitions.keys())\n",
    "        idxs.sort()\n",
    "        assert(idxs[0] <= idxs[-1])\n",
    "        for ix in idxs:\n",
    "            f.write(\" \".join([str(e) for e in partitions[ix]]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!for graph in graphs/*.graph; do echo \"time start\"; time timeout 5m ./a.out  --G_phi=0.05 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph | tee \"$graph.out\" ; echo \"time end\"; done\n",
    "#!SHELL=/bin/bash\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def run_decomp(graph_files):\n",
    "    for graph in graph_files:\n",
    "        print(graph)\n",
    "\n",
    "        #run decomposition on all graphs with time limit\n",
    "        out_path = graph + \".out\"\n",
    "        !time -p timeout 30m ./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph > \"$out_path\"\n",
    "\n",
    "        #TODO return code\n",
    "        if not glob.glob(graph + \"cut.txt\"):\n",
    "            print(\"decomp on\", graph, \"did not finish in time\")\n",
    "            continue\n",
    "\n",
    "        #how many clusters, k, did we get?\n",
    "        with open(out_path) as f:\n",
    "            lns = f.readlines()\n",
    "            cluster_line = next(l for l in lns if \"n clusters\" in l)\n",
    "            cluster_line.strip(\"\\n\")\n",
    "            n_clusters = int(cluster_line.split(\";\")[1])\n",
    "\n",
    "        if n_clusters == 1:\n",
    "            print(\"No cut found, continue\")\n",
    "            continue\n",
    "\n",
    "        print(\"n clusters found\", n_clusters)\n",
    "        #run metis on k\n",
    "        metis_stdio_path = graph + \".out.metis\"\n",
    "        !/usr/local/bin/gpmetis -ufactor=1000 $graph $n_clusters -contig >  $metis_stdio_path\n",
    "\n",
    "        metis_file = graph + \".part.\" + str(n_clusters)\n",
    "        decomp_file = graph + \"cut.txt\"\n",
    "        \n",
    "        metis_partition_file_converter(metis_file, metis_file)\n",
    "\n",
    "        rw_graph      = graph + \".row_whole\"\n",
    "        rw_file_ours  = graph + \".rw_ours\"\n",
    "        rw_file_metis = graph + \".rw_metis\"\n",
    "\n",
    "        #!./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander  > \"$rw_graph\"\n",
    "        #!./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander -p \"$decomp_file\" > \"$rw_file_ours\"\n",
    "        #!./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander -p \"$metis_file\"  > \"$rw_file_metis\"\n",
    "    \n",
    "        bname = os.path.basename(graph).split(\".\")[0]\n",
    "        !mkdir -p results/\"$bname\"\n",
    "        for f in glob.glob(\"\".join(graph.split(\".\")[:-1]) + \"*\"):\n",
    "            if f != graph:\n",
    "                !mv $f results/\"$bname\"/\n",
    "\n",
    "\n",
    "#graph_files = glob.glob(\"graphs/*.graph\")\n",
    "#graph_files.sort()\n",
    "#run_decomp(graph_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "#from IPython.display import Javascript, display\n",
    "#display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)'))\n",
    "\n",
    "\n",
    "#Javascript('IPython.notebook.execute_cells_below()')\n",
    "#button = w.Button(button_style='info',description=\"Execute `Report\")\n",
    "#button.on_click(run_all)\n",
    "#display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "def test_convergence(matrix, threshold):\n",
    "    #np.set_printoptions(threshold=np.inf)    \n",
    "    n = len(matrix)\n",
    "    assert(len(matrix[0]) == n)\n",
    "    e = sum([sum(row) for row in matrix])\n",
    "    print(\"n;\", n)\n",
    "    print(\"e;\", e)\n",
    "    #matrix = np.array(matrix)\n",
    "\n",
    " \n",
    "\n",
    "    colsums = [sum([matrix[i][j] for j in range(n)]) for i in range(n)]\n",
    "    uniform = np.array([colsums[i]/e for i in range(n)], dtype=np.float64)\n",
    "    #uniform = np.array([1] + [0 for _ in range(n - 1)]) #np.array([sum([e for e in matrix[i]])/e for i in range(n)])\n",
    "    assert(0.99 <=sum(uniform) <= 1.01)\n",
    "    walk = np.array([1.] + [0 for _ in range(n - 1)], np.float64) #Seed?\n",
    "    #walk = np.array([1./n for _ in range(n)])\n",
    "\n",
    "    sums = [sum(matrix[i]) for i in range(n)]\n",
    "    for s in sums:\n",
    "        pass #assert(s > 0)\n",
    "    matrix_with_drift = [[e/sums[j] for i, e in enumerate(subl)] for j, subl in enumerate(matrix)]\n",
    "\n",
    "    assert(len(matrix_with_drift) == n)\n",
    "    assert(len(matrix_with_drift[0]) == n)\n",
    "\n",
    "    for i in range(n):\n",
    "        matrix_with_drift[i][i] = sum(matrix_with_drift[i])\n",
    "        assert(matrix_with_drift[i][i] > 0)\n",
    "        cnt = sum(matrix_with_drift[i])\n",
    "        matrix_with_drift[i] = [m/cnt for m in matrix_with_drift[i]]\n",
    "        assert(0.99 <= sum(matrix_with_drift[i]) <= 1.01)\n",
    "\n",
    "    matrix = matrix_with_drift\n",
    "    matrix = np.array(matrix, np.float64)\n",
    "\n",
    "    dist  = np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) / n\n",
    "    steps = 0\n",
    "\n",
    "\n",
    "    while dist > threshold and steps < 10000: #and steps < len(matrix):\n",
    "        walk = np.matmul(matrix.T, walk.T)\n",
    "        assert(0.99 < sum(walk)    < 1.01)\n",
    "        assert(0.99 < sum(uniform) < 1.01)\n",
    "        steps += 1\n",
    "        dist = np.linalg.norm(walk - uniform, 2)\n",
    "    try:\n",
    "        s_matrix     = sparse.coo_matrix(matrix)\n",
    "        eigenvecs, _ = sparse.linalg.eigs(s_matrix, k=2)\n",
    "\n",
    "        print(\"eigenvecs sample;\",     eigenvecs[:5])\n",
    "        print(\"first two eigvenvals;\", eigenvecs[eigenvecs.argsort()[-2:][::-1]])\n",
    "    except:\n",
    "        print(\"Eigenvalue calculation failed\")\n",
    "\n",
    "    print(\"dist;\", dist)\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def graph_and_cut_to_numpy(gf, cf):\n",
    "\n",
    "    graph = {}\n",
    "    with open(gf, \"r\") as f:\n",
    "        first_row = f.readline().strip(\"\\n\").split(\" \")\n",
    "        while [] in first_row:\n",
    "            first_row.remove([])\n",
    "        n, e = [int(e) for e in first_row]\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            graph[i + 1] = []\n",
    "            for v in row.strip(\"\\n\").split(\" \"):\n",
    "                if v == \"\":\n",
    "                    continue\n",
    "                assert(int(v))\n",
    "                graph[i + 1].append(int(v))\n",
    "                try:\n",
    "                    graph[v].append(int(i + 1))\n",
    "                except KeyError:\n",
    "                    graph[v] = [int(i + 1)]\n",
    "\n",
    "    clusters = []\n",
    "    #TOFIX: clusters are 0 index\n",
    "    with open(cf, \"r\") as f:\n",
    "        n_counter = 0\n",
    "        e_counter = 0\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            clusters.append([int(e) + 1 for e in row.strip(\"\\n\").split(\" \") if e != \"\"])\n",
    "            #print(clusters[-1])\n",
    "            n_counter += 1\n",
    "        e_counter += len(clusters[-1])\n",
    "\n",
    "\n",
    "    return graph, clusters\n",
    "\n",
    "\n",
    "def test(graph_f, cut_f):\n",
    "    graph, cuts = graph_and_cut_to_numpy(graph_f, cut_f)\n",
    "    #graph, cuts = graph_and_cut_to_numpy(\"graphs/whitaker3.graph\", \"old_results/whitaker3.graph.part.94.chaco\")\n",
    "    subgs = []\n",
    "\n",
    "    for clidx, cut in enumerate(cuts):\n",
    "\n",
    "        v_set = set()\n",
    "        for el in cut:\n",
    "            v_set.add(int(el))\n",
    "\n",
    "        #print(v_set)\n",
    "        n = len(v_set)\n",
    "\n",
    "        subg = {k:[] for k in cut}\n",
    "        #print(\"first element in cut\", cut[0])\n",
    "        for k in graph:\n",
    "            for v in graph[k]:\n",
    "                if v in v_set and k in v_set:\n",
    "                    subg[k].append(v)\n",
    "\n",
    "\n",
    "        rekey_dict    = {v:i for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "        rekey_reverse = {i:v for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "\n",
    "        success = True\n",
    "        new_subg = [[0 for _ in range(n)] for _ in range(n)]\n",
    "        for k in subg: \n",
    "            #assert(len(subg[k]) > 0)\n",
    "            if len(subg[k]) == 0:\n",
    "                print(\"WARNING - disconnected cluster;\", clidx)\n",
    "                print(subg[k])\n",
    "                print(subg)\n",
    "                success = False\n",
    "                break\n",
    "            for v in subg[k]:\n",
    "                new_subg[rekey_dict[k]][rekey_dict[v]] = 1\n",
    "                new_subg[rekey_dict[v]][rekey_dict[k]] = 1\n",
    "        if not success:\n",
    "            continue\n",
    "        assert(len(new_subg) == n)\n",
    "        assert(len(new_subg[0]) == n)\n",
    "        steps = test_convergence(new_subg, 0.01)\n",
    "        print(\"Cluster\", clidx, \"of size: \", n, \"took n steps to converge;\", steps)\n",
    "        import math\n",
    "        print(\"steps/ log2 nodes;\", steps/math.log2(n))\n",
    "\n",
    "        #Scipy and numpy can't calculate eigenvalues quick\n",
    "        \n",
    "        n = len(graph)\n",
    "        if n <= 5000:\n",
    "            trivial_subg = [[0 for _ in range(n)] for _ in range(n)]\n",
    "            for i, k in enumerate(graph):\n",
    "                for j, _ in enumerate(graph[k]):\n",
    "                    trivial_subg[i][j] = 1\n",
    "            steps = test_convergence(trivial_subg, 0.01)\n",
    "            print(\"Whole graph took n steps to converge:\", steps)\n",
    "            import math\n",
    "            print(\"steps/ log2 nodes\", steps/math.log2(n))\n",
    "        \n",
    "\n",
    "#test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for graph in graphs/*.graph; do echo \"time start\"; time timeout 5m ./a.out  --G_phi=0.05 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph | tee \"$graph.out\" ; echo \"time end\"; done\n",
    "#!SHELL=/bin/bash\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def run_decomp(graph_files):\n",
    "    for graph in graph_files:\n",
    "        print(\"decomp on;\", graph)\n",
    "        print(graph)\n",
    "\n",
    "        #run decomposition on all graphs with time limit\n",
    "        out_path = graph + \".out\"\n",
    "        !time -p timeout 15m ./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph > \"$out_path\"\n",
    "\n",
    "        #TODO return code\n",
    "        if not glob.glob(graph + \"cut.txt\"):\n",
    "            print(\"decomp on\", graph, \"did not finish in time\")\n",
    "            continue\n",
    "\n",
    "        #how many clusters, k, did we get?\n",
    "        with open(out_path) as f:\n",
    "            lns = f.readlines()\n",
    "            cluster_line = next(l for l in lns if \"n clusters\" in l)\n",
    "            cluster_line.strip(\"\\n\")\n",
    "            n_clusters = int(cluster_line.split(\";\")[1])\n",
    "\n",
    "        if n_clusters == 1:\n",
    "            print(\"No cut found, continue\")\n",
    "            continue\n",
    "\n",
    "        print(\"n clusters found\", n_clusters)\n",
    "        #run metis on k\n",
    "        metis_stdio_path = graph + \".out.metis\"\n",
    "        !/usr/local/bin/gpmetis -ufactor=1000 $graph $n_clusters -contig >  $metis_stdio_path\n",
    "\n",
    "        metis_file = graph + \".part.\" + str(n_clusters)\n",
    "        decomp_file = graph + \"cut.txt\"\n",
    "        \n",
    "        metis_partition_file_converter(metis_file, metis_file)\n",
    "\n",
    "        rw_graph      = graph + \".row_whole\"\n",
    "        rw_file_ours  = graph + \".rw_ours\"\n",
    "        rw_file_metis = graph + \".rw_metis\"\n",
    "\n",
    "        #!./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander  > \"$rw_graph\"\n",
    "        #!./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander -p \"$decomp_file\" > \"$rw_file_ours\"\n",
    "        #!./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander -p \"$metis_file\"  > \"$rw_file_metis\"\n",
    "        #TOFIX This should be saving to file!\n",
    "        print(\"decomp ours\")\n",
    "        test(graph, decomp_file)\n",
    "        print(\"decomp metis\")\n",
    "        test(graph, metis_file)        \n",
    "\n",
    "        bname = os.path.basename(graph).split(\".\")[0]\n",
    "        !mkdir -p results/\"$bname\"\n",
    "        for f in glob.glob(\"\".join(graph.split(\".\")[:-1]) + \"*\"):\n",
    "            if f != graph:\n",
    "                !mv $f results/\"$bname\"/\n",
    "\n",
    "\n",
    "#graph_files = glob.glob(\"graphs/*.graph\")\n",
    "#graph_files.sort()\n",
    "#run_decomp(graph_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run_decomp([\"synthetic/random_3_regular_5000.graph\"])\n",
    "#run_decomp([\"graphs/crack.graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "g1s = list(glob.glob(\"synthetic/*\"))\n",
    "run_decomp(g1s)\n",
    "g2s = list(glob.glob(\"graphs/*\"))\n",
    "run_decomp(g2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#data\n",
    "#placeholder\n",
    "data = [0.6, 0.5, 0.7]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "plt.plot(data)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def metis_partition_file_converter(inp, out):\n",
    "    import sys\n",
    "\n",
    "    #inp = sys.argv[1]\n",
    "    #out = sys.argv[2]\n",
    "\n",
    "    partitions = {}\n",
    "\n",
    "    def add(el, idx):\n",
    "        if idx not in partitions:\n",
    "            partitions[idx] = [el]\n",
    "        else:\n",
    "            partitions[idx].append(el)\n",
    "\n",
    "\n",
    "    with open(inp, \"r+\") as f:\n",
    "        el = 0\n",
    "        for line in f:\n",
    "            idx = int(line.strip())\n",
    "            add(el, idx)\n",
    "            el += 1\n",
    "\n",
    "    with open(out, \"w+\") as f:\n",
    "        idxs = list(partitions.keys())\n",
    "        idxs.sort()\n",
    "        assert(idxs[0] <= idxs[-1])\n",
    "        for ix in idxs:\n",
    "            f.write(\" \".join([str(e) for e in partitions[ix]]) + \"\\n\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!for graph in graphs/*.graph; do echo \"time start\"; time timeout 5m ./a.out  --G_phi=0.05 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph | tee \"$graph.out\" ; echo \"time end\"; done\n",
    "#!SHELL=/bin/bash\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#%%javascript\n",
    "#from IPython.display import Javascript, display\n",
    "#display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)'))\n",
    "\n",
    "\n",
    "#Javascript('IPython.notebook.execute_cells_below()')\n",
    "#button = w.Button(button_style='info',description=\"Execute `Report\")\n",
    "#button.on_click(run_all)\n",
    "#display(button)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "def test_convergence(matrix, threshold):\n",
    "    #np.set_printoptions(threshold=np.inf)    \n",
    "    #n = len(matrix)\n",
    "    n = matrix.shape[0]\n",
    "    #assert(len(matrix[0]) == n)\n",
    "\n",
    "    #e = sum([sum(row) for row in matrix])\n",
    "    e = matrix.sum()\n",
    "\n",
    "    print(\"n;\", n)\n",
    "    print(\"e;\", e)\n",
    "    #matrix = np.array(matrix)\n",
    "\n",
    " \n",
    "    #colsums = [sum([matrix[i][j] for j in range(n)]) for i in range(n)]\n",
    "    colsums = matrix.sum(axis=1)\n",
    "    #uniform = np.array([colsums[i]/e for i in range(n)], dtype=np.float64)\n",
    "    uniform = np.zeros(shape=(n,))\n",
    "    #print(colsums)\n",
    "    for i in range(n):\n",
    "        uniform[i] = colsums[i]/e\n",
    "    \n",
    "    #uniform = np.array([1] + [0 for _ in range(n - 1)]) #np.array([sum([e for e in matrix[i]])/e for i in range(n)])\n",
    "    assert(0.99 <= uniform.sum() <= 1.01)\n",
    "\n",
    "    #scipy matmul numpy : A.dot(v)\n",
    "\n",
    "    walk = np.array([1.] + [0 for _ in range(n - 1)], np.float64) #Seed?\n",
    "    #walk = np.array([1./n for _ in range(n)])\n",
    "\n",
    "    #sums = [sum(matrix[i]) for i in range(n)]\n",
    "    sums = matrix.sum(axis=0)\n",
    "    assert((sums != 0).any())\n",
    "\n",
    "    for s in sums:\n",
    "        pass #assert(s > 0)\n",
    "    \n",
    "    #No longer sparse ?\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if matrix[i, j] != 0:\n",
    "                matrix[i, j] = matrix[i, j] / int(sums[:, j])\n",
    "\n",
    "    #matrix_with_drift = [[e/sums[j] for i, e in enumerate(subl)] for j, subl in enumerate(matrix)]\n",
    "\n",
    "    #assert(len(matrix) == n)\n",
    "    #assert(len(matrix[0]) == n)\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        matrix[i, i] = matrix[i, :].sum()\n",
    "        assert(matrix[i, i] > 0)\n",
    "        cnt = matrix[i, :].sum()\n",
    "        matrix[i, :] /= cnt\n",
    "\n",
    "        assert(0.99 <= matrix[i, :].sum() <= 1.01)\n",
    "\n",
    "    #matrix = np.array(matrix, np.float64)\n",
    "    matrix = sparse.coo_matrix(matrix)\n",
    "\n",
    "    dist = np.linalg.norm(walk - uniform, 1)\n",
    "    #dist  = np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) / n\n",
    "    steps = 0\n",
    "    while dist > threshold and steps < 10000: #and steps < len(matrix):\n",
    "        walk = matrix.T.dot(walk.T)\n",
    "        assert(0.99 < sum(walk)    < 1.01)\n",
    "        assert(0.99 < sum(uniform) < 1.01)\n",
    "        steps += 1\n",
    "        dist = np.linalg.norm(walk - uniform, 1)\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s_matrix     = sparse.coo_matrix(matrix)\n",
    "        eigenvecs, _ = sparse.linalg.eigs(s_matrix, k=2)\n",
    "\n",
    "        print(\"eigenvecs sample;\",     eigenvecs[:5])\n",
    "        print(\"first two eigvenvals;\", eigenvecs[eigenvecs.argsort()[-2:][::-1]])\n",
    "    except:\n",
    "        print(\"Eigenvalue calculation failed\")\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"dist;\", dist)\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def graph_and_cut_to_numpy(gf, cf):\n",
    "\n",
    "    graph = {}\n",
    "    with open(gf, \"r\") as f:\n",
    "        first_row = f.readline().strip(\"\\n\").split(\" \")\n",
    "        while [] in first_row:\n",
    "            first_row.remove([])\n",
    "        n, e = [int(e) for e in first_row]\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            graph[i + 1] = []\n",
    "            for v in row.strip(\"\\n\").split(\" \"):\n",
    "                if v == \"\":\n",
    "                    continue\n",
    "                assert(int(v))\n",
    "                graph[i + 1].append(int(v))\n",
    "                try:\n",
    "                    graph[v].append(int(i + 1))\n",
    "                except KeyError:\n",
    "                    graph[v] = [int(i + 1)]\n",
    "\n",
    "    clusters = []\n",
    "    #TOFIX: clusters are 0 index\n",
    "    with open(cf, \"r\") as f:\n",
    "        n_counter = 0\n",
    "        e_counter = 0\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            clusters.append([int(e) + 1 for e in row.strip(\"\\n\").split(\" \") if e != \"\"])\n",
    "            #print(clusters[-1])\n",
    "            n_counter += 1\n",
    "        e_counter += len(clusters[-1])\n",
    "\n",
    "\n",
    "    return graph, clusters\n",
    "\n",
    "\n",
    "def test(graph_f, cut_f):\n",
    "    graph, cuts = graph_and_cut_to_numpy(graph_f, cut_f)\n",
    "    #graph, cuts = graph_and_cut_to_numpy(\"graphs/whitaker3.graph\", \"old_results/whitaker3.graph.part.94.chaco\")\n",
    "    subgs = []\n",
    "\n",
    "    for clidx, cut in enumerate(cuts):\n",
    "\n",
    "        v_set = set()\n",
    "        for el in cut:\n",
    "            v_set.add(int(el))\n",
    "\n",
    "        #print(v_set)\n",
    "        n = len(v_set)\n",
    "\n",
    "        subg = {k:[] for k in cut}\n",
    "        #print(\"first element in cut\", cut[0])\n",
    "        for k in graph:\n",
    "            for v in graph[k]:\n",
    "                if v in v_set and k in v_set:\n",
    "                    subg[k].append(v)\n",
    "\n",
    "\n",
    "        rekey_dict    = {v:i for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "        rekey_reverse = {i:v for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "\n",
    "        success = True\n",
    "        #new_subg = [[0 for _ in range(n)] for _ in range(n)]\n",
    "        new_subg = sparse.dok_matrix((n, n))\n",
    "\n",
    "        for k in subg: \n",
    "\n",
    "            if len(subg[k]) == 0:\n",
    "                print(\"WARNING - disconnected cluster;\", clidx, \"number of nodes in subgraph is;\", len(cut))\n",
    "                success = False\n",
    "                break\n",
    "\n",
    "            for v in subg[k]:\n",
    "                #new_subg[rekey_dict[k]][rekey_dict[v]] = 1\n",
    "                #new_subg[rekey_dict[v]][rekey_dict[k]] = 1\n",
    "                new_subg[rekey_dict[k], rekey_dict[v]] = 1\n",
    "                new_subg[rekey_dict[v], rekey_dict[k]] = 1\n",
    "\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "\n",
    "        steps = test_convergence(new_subg, 0.01)\n",
    "\n",
    "        print(\"Cluster\", clidx, \"of size: \", n, \"took n steps to converge;\", steps)\n",
    "\n",
    "        import math\n",
    "        print(\"steps/ log2 nodes;\", steps/math.log2(n))\n",
    "\n",
    "        #Scipy and numpy can't calculate eigenvalues quick\n",
    "        \n",
    "        n = len(graph)\n",
    "    if True: #n <= 5000\n",
    "        trivial_subg = sparse.dok_matrix((n, n))\n",
    "        for i, k in enumerate(graph):\n",
    "            for j, _ in enumerate(graph[k]):\n",
    "                #trivial_subg[i][j] = 1\n",
    "                trivial_subg[i, j] = 1\n",
    "\n",
    "        steps = test_convergence(trivial_subg, 0.01)\n",
    "        print(\"Whole graph took n steps to converge:\", steps)\n",
    "        import math\n",
    "        print(\"steps/ log2 nodes\", steps/math.log2(n))\n",
    "        \n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def run_decomp(graph_files):\n",
    "    for graph in graph_files:\n",
    "        print(\"decomp on;\", graph)\n",
    "        print(graph)\n",
    "\n",
    "        #run decomposition on all graphs with time limit\n",
    "        out_path = graph + \".out\"\n",
    "        get_ipython().system('time -p timeout 30m ./a.out  --G_phi=0.01 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph > \"$out_path\"')\n",
    "\n",
    "        #TODO return code\n",
    "        if not glob.glob(graph + \"cut.txt\"):\n",
    "            print(\"decomp on\", graph, \"did not finish in time\")\n",
    "            continue\n",
    "\n",
    "        #how many clusters, k, did we get?\n",
    "        with open(out_path) as f:\n",
    "            lns = f.readlines()\n",
    "            cluster_line = next(l for l in lns if \"n clusters\" in l)\n",
    "            cluster_line.strip(\"\\n\")\n",
    "            n_clusters = int(cluster_line.split(\";\")[1])\n",
    "\n",
    "        if n_clusters <= 1:\n",
    "            print(\"No cut found, continue\")\n",
    "            continue\n",
    "\n",
    "        print(\"n clusters found\", n_clusters)\n",
    "        #run metis on k\n",
    "        metis_stdio_path = graph + \".out.metis\"\n",
    "        get_ipython().system('/usr/bin/gpmetis -ufactor=1000 $graph $n_clusters -contig >  $metis_stdio_path')\n",
    "\n",
    "        metis_file = graph + \".part.\" + str(n_clusters)\n",
    "        decomp_file = graph + \"cut.txt\"\n",
    "        \n",
    "        metis_partition_file_converter(metis_file, metis_file)\n",
    "\n",
    "        rw_graph      = graph + \".row_whole\"\n",
    "        rw_file_ours  = graph + \".rw_ours\"\n",
    "        rw_file_metis = graph + \".rw_metis\"\n",
    "\n",
    "        #TOFIX This should be saving to file!\n",
    "        print(\"decomp ours\")\n",
    "        test(graph, decomp_file)\n",
    "        print(\"decomp metis\")\n",
    "        test(graph, metis_file)        \n",
    "\n",
    "        bname = os.path.basename(graph).split(\".\")[0]\n",
    "        get_ipython().system('mkdir -p results/\"$bname\"')\n",
    "        for f in glob.glob(\"\".join(graph.split(\".\")[:-1]) + \"*\"):\n",
    "            if f != graph:\n",
    "                get_ipython().system('mv $f results/\"$bname\"/')\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "g1s = list(glob.glob(\"synthetic/*\"))\n",
    "run_decomp(g1s)\n",
    "g2s = list(glob.glob(\"graphs/*\"))\n",
    "run_decomp(g2s)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
