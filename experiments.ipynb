{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from os import path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "\n",
    "def metis_partition_file_converter(inp, out):\n",
    "    import sys\n",
    "\n",
    "\n",
    "    partitions = {}\n",
    "\n",
    "    def add(el, idx):\n",
    "        if idx not in partitions:\n",
    "            partitions[idx] = [el]\n",
    "        else:\n",
    "            partitions[idx].append(el)\n",
    "\n",
    "\n",
    "    with open(inp, \"r+\") as f:\n",
    "        el = 0\n",
    "        for line in f:\n",
    "            idx = int(line.strip())\n",
    "            add(el, idx)\n",
    "            el += 1\n",
    "\n",
    "    with open(out, \"w+\") as f:\n",
    "        idxs = list(partitions.keys())\n",
    "        idxs.sort()\n",
    "        assert(idxs[0] <= idxs[-1])\n",
    "        for ix in idxs:\n",
    "            f.write(\" \".join([str(e) for e in partitions[ix]]) + \"\\n\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def graph_and_cut_to_numpy(gf, cf):\n",
    "\n",
    "    graph = {}\n",
    "    with open(gf, \"r\") as f:\n",
    "        first_row = f.readline().strip(\"\\n\").split(\" \")\n",
    "        while [] in first_row:\n",
    "            first_row.remove([])\n",
    "        n, e = [int(e) for e in first_row]\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            graph[i + 1] = []\n",
    "            for v in row.strip(\"\\n\").split(\" \"):\n",
    "                if v == \"\":\n",
    "                    continue\n",
    "                assert(int(v))\n",
    "                graph[i + 1].append(int(v))\n",
    "                try:\n",
    "                    graph[v].append(int(i + 1))\n",
    "                except KeyError:\n",
    "                    graph[v] = [int(i + 1)]\n",
    "\n",
    "    clusters = []\n",
    "    #TOFIX: clusters are 0 index\n",
    "    with open(cf, \"r\") as f:\n",
    "        n_counter = 0\n",
    "        e_counter = 0\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            clusters.append([int(e) + 1 for e in row.strip(\"\\n\").split(\" \") if e != \"\"])\n",
    "            #print(clusters[-1])\n",
    "            n_counter += 1\n",
    "        e_counter += len(clusters[-1])\n",
    "\n",
    "\n",
    "    return graph, clusters\n",
    "\n",
    "\n",
    "def test(graph_f, cut_f):\n",
    "    graph, cuts = graph_and_cut_to_numpy(graph_f, cut_f)\n",
    "    #graph, cuts = graph_and_cut_to_numpy(\"graphs/whitaker3.graph\", \"old_results/whitaker3.graph.part.94.chaco\")\n",
    "    subgs = []\n",
    "\n",
    "    for clidx, cut in enumerate(cuts):\n",
    "\n",
    "        print(\"Start working on cluster;\", clidx)\n",
    "        if len(cut) == 1:\n",
    "            print(\"Cut len is 1, continue\")\n",
    "            continue\n",
    "\n",
    "        v_set = set()\n",
    "        for el in cut:\n",
    "            v_set.add(int(el))\n",
    "\n",
    "        #print(v_set)\n",
    "        n = len(v_set)\n",
    "\n",
    "        subg = {k:[] for k in cut}\n",
    "        #print(\"first element in cut\", cut[0])\n",
    "        for k in graph:\n",
    "            for v in graph[k]:\n",
    "                if v in v_set and k in v_set:\n",
    "                    subg[k].append(v)\n",
    "\n",
    "\n",
    "        rekey_dict    = {v:i for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "        rekey_reverse = {i:v for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "\n",
    "        success = True\n",
    "        #new_subg = [[0 for _ in range(n)] for _ in range(n)]\n",
    "        new_subg = sparse.dok_matrix((n, n))\n",
    "\n",
    "        for k in subg: \n",
    "\n",
    "            if len(subg[k]) == 0:\n",
    "                print(\"WARNING - disconnected cluster;\", clidx, \"number of nodes in subgraph is;\", len(cut))\n",
    "                success = False\n",
    "                break\n",
    "\n",
    "            for v in subg[k]:\n",
    "                #new_subg[rekey_dict[k]][rekey_dict[v]] = 1\n",
    "                #new_subg[rekey_dict[v]][rekey_dict[k]] = 1\n",
    "                new_subg[rekey_dict[k], rekey_dict[v]] = 1\n",
    "                new_subg[rekey_dict[v], rekey_dict[k]] = 1\n",
    "\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "\n",
    "        steps = test_convergence(new_subg, 0.01)\n",
    "\n",
    "        print(\"Cluster\", clidx, \"of size: \", n, \"took n steps to converge;\", steps)\n",
    "\n",
    "        import math\n",
    "        print(\"steps/ log2 nodes;\", steps/math.log2(n))\n",
    "\n",
    "        #Scipy and numpy can't calculate eigenvalues quick\n",
    "        \n",
    "        n = len(graph)\n",
    "    if True: #n <= 5000\n",
    "        trivial_subg = sparse.dok_matrix((n, n))\n",
    "        for i, k in enumerate(graph):\n",
    "            for j, _ in enumerate(graph[k]):\n",
    "                #trivial_subg[i][j] = 1\n",
    "                trivial_subg[i, j] = 1\n",
    "\n",
    "        steps = test_convergence(trivial_subg, 0.01)\n",
    "        print(\"Whole graph took n steps to converge:\", steps)\n",
    "        import math\n",
    "        print(\"steps/ log2 nodes\", steps/math.log2(n))\n",
    "        \n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def run_decomp(graph_files):\n",
    "    for graph in graph_files:\n",
    "        print(\"decomp on;\", graph)\n",
    "        print(graph)\n",
    "\n",
    "        #run decomposition on all graphs with time limit\n",
    "        out_path = graph + \".out\"\n",
    "        #os.system\n",
    "        #subprocess.check_call('time -p timeout 15m ./a.out  -S --G_phi=0.01 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph > \"$out_path\"', shell=True)\n",
    "        try:\n",
    "            subprocess.check_call(\"time -p timeout 15m ./a.out  -S --G_phi=0.01 --H_phi=0.4 --vol=1 --h_ratio=0. -f \" +  graph + \" > \" + out_path, shell=True)\n",
    "        except:\n",
    "            #Too long time?\n",
    "            continue\n",
    "\n",
    "        #TODO return code\n",
    "        if not glob.glob(graph + \"cut.txt\"):\n",
    "            print(\"decomp on\", graph, \"did not finish in time\")\n",
    "            continue\n",
    "\n",
    "        #how many clusters, k, did we get?\n",
    "        with open(out_path) as f:\n",
    "            lns = f.readlines()\n",
    "            cluster_line = next(l for l in lns if \"n clusters\" in l)\n",
    "            cluster_line.strip(\"\\n\")\n",
    "            n_clusters = int(cluster_line.split(\";\")[1])\n",
    "\n",
    "        if n_clusters <= 1:\n",
    "            print(\"No cut found, continue\")\n",
    "            continue\n",
    "\n",
    "        print(\"n clusters found\", n_clusters)\n",
    "        #run metis on k\n",
    "        metis_stdio_path = graph + \".out.metis\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            subprocess.check_call(\"/usr/bin/gpmetis -ufactor=1000 \" +  graph + \" \" + str(n_clusters) + \" -contig >  \" + metis_stdio_path, shell=True)\n",
    "            metis_file = graph + \".part.\" + str(n_clusters)\n",
    "            decomp_file = graph + \"cut.txt\"\n",
    "            \n",
    "            metis_partition_file_converter(metis_file, metis_file)\n",
    "            print(\"decomp metis\")\n",
    "            test(graph, metis_file)\n",
    "        except Exception as e:\n",
    "            print(\"METIS failed\", e)\n",
    "\n",
    "        rw_graph      = graph + \".row_whole\"\n",
    "        rw_file_ours  = graph + \".rw_ours\"\n",
    "        rw_file_metis = graph + \".rw_metis\"\n",
    "\n",
    "        #TOFIX This should be saving to file!\n",
    "        print(\"decomp ours\")\n",
    "        test(graph, decomp_file)\n",
    "        print(\"decomp metis\")\n",
    "        test(graph, metis_file)        \n",
    "\n",
    "        bname = os.path.basename(graph).split(\".\")[0]\n",
    "        os.system('mkdir -p results/\"$bname\"')\n",
    "        for f in glob.glob(\"\".join(graph.split(\".\")[:-1]) + \"*\"):\n",
    "            if f != graph:\n",
    "                os.system('mv $f results/\"$bname\"/')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import glob\n",
    "\n",
    "\n",
    "g1s = list(glob.glob(\"synthetic/*\"))\n",
    "run_decomp(g1s)\n",
    "g2s = list(glob.glob(\"graphs/*\"))\n",
    "run_decomp(g2s)\n",
    "\"\"\"\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse as sparse\n",
    "import torch\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "def test_convergence(matrix, threshold):\n",
    "\n",
    "    n = matrix.shape[0]\n",
    "    e = matrix.sum()\n",
    "\n",
    "    print(\"n;\", n)\n",
    "    print(\"e;\", e)\n",
    "\n",
    "    #Number of in-links\n",
    "    #Axii are inverted for some reason\n",
    "    colsums = [i.item() for i in scipy.array((matrix.sum(axis=0))).T]\n",
    "\n",
    "    uniform = np.zeros(shape=(n,))\n",
    "\n",
    "    #Stationary distribution is uniform normalized by in-links\n",
    "    for i in range(n):\n",
    "        uniform[i] = colsums[i]/e\n",
    "    \n",
    "    assert(0.99 <= uniform.sum() <= 1.01)\n",
    "    assert(-0.01 < sum(colsums) - e < 0.01)\n",
    "\n",
    "    #TODO seed the random start\n",
    "    walk = np.array([1.] + [0 for _ in range(n - 1)], np.float64)\n",
    "    walk = torch.FloatTensor(walk)\n",
    "\n",
    "    #Every node has positive in and out link\n",
    "    sums = matrix.sum(axis=1)\n",
    "    assert((sums != 0).any())\n",
    "    sums = matrix.sum(axis=0)\n",
    "    assert((sums != 0).any())\n",
    "\n",
    "    rowsums = matrix.sum(axis=1)\n",
    "    for i in range(n):\n",
    "        matrix[i, i] = rowsums[i].item()\n",
    "        cnt = rowsums[i] * 2\n",
    "        matrix[i] /= cnt\n",
    "        #assert(0.999 <= matrix[i].sum() <= 1.001)\n",
    "\n",
    "    matrix = matrix.tocoo()   \n",
    "    values = matrix.data\n",
    "    indices = np.vstack((matrix.row, matrix.col))\n",
    "\n",
    "    #Number of edges and self-loops\n",
    "    assert(len(matrix.row) == len(matrix.col) == e + n)\n",
    "\n",
    "    matrix = torch.sparse.FloatTensor(torch.LongTensor(indices), torch.FloatTensor(values), torch.Size(matrix.shape))\n",
    "\n",
    "    all_ones = torch.FloatTensor([1. for n in range(n)])\n",
    "\n",
    "    assert(torch.allclose(all_ones, torch.sparse.sum(matrix, dim=1).to_dense()))\n",
    "\n",
    "    uniform = torch.from_numpy(uniform)\n",
    "    dist = torch.norm(walk - uniform, 1)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    if torch.cuda.is_available():        \n",
    "        matrix   = matrix.to(\"cuda\")\n",
    "        walk     = walk.to(\"cuda\")\n",
    "        uniform  = uniform.to(\"cuda\")\n",
    "        all_ones = all_ones.to(\"cuda\")\n",
    "\n",
    "    print(\"Start walk\")\n",
    "\n",
    "\n",
    "    step_lim = n**2\n",
    "    print_max = 15\n",
    "    timeout_minutes = 10\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    while dist > threshold and steps < step_lim: \n",
    "        #Why is this transposed?\n",
    "        walk = torch.sparse.mm(matrix.transpose(1, 0), walk.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        assert(0.99  < torch.sum(walk)    < 1.01)\n",
    "        assert(0.99  < torch.sum(uniform) < 1.01)\n",
    "        assert(-0.02 < torch.sum(dist)    < 2.02)\n",
    "        assert(0.99  < torch.sum(uniform) < 1.01)\n",
    "        assert(-0.01 < torch.sum(walk - uniform) < 0.01)\n",
    "\n",
    "        steps += 1\n",
    "        assert(walk.shape == uniform.shape)\n",
    "        dist = torch.norm(walk - uniform, 1)\n",
    "\n",
    "        if steps % (step_lim // print_max) == 0:\n",
    "            print(\"Dist at\", steps, \" = \", dist)\n",
    "            #print(\"Walk - uniform\", walk - uniform)\n",
    "\n",
    "        if (time.time() - time_start) > timeout_minutes:\n",
    "            print(\"No convergence! \")\n",
    "            return -1\n",
    "\n",
    "    print(\"dist;\", dist)\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "g1s = list(glob.glob(\"synthetic/*\"))\n",
    "g1s.sort()\n",
    "run_decomp(g1s)\n",
    "g2s = list(glob.glob(\"graphs/*\"))\n",
    "g2s.sort()\n",
    "run_decomp(g2s)\n",
    "\"\"\"\n",
    "gls = list(glob.glob(\"results/*/*graphcut*\"))\n",
    "gls.sort()\n",
    "for gcut in gls:\n",
    "    bname = os.path.basename(gcut)\n",
    "    graph = os.path.join(\"graphs/\", bname[:-7]) #x.graphcut.txt -> x.graph\n",
    "    test(graph, gcut)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
