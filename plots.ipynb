{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#placeholder\n",
    "data = [0.6, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metis_partition_file_converter(inp, out):\n",
    "    import sys\n",
    "\n",
    "    #inp = sys.argv[1]\n",
    "    #out = sys.argv[2]\n",
    "\n",
    "    partitions = {}\n",
    "\n",
    "    def add(el, idx):\n",
    "        if idx not in partitions:\n",
    "            partitions[idx] = [el]\n",
    "        else:\n",
    "            partitions[idx].append(el)\n",
    "\n",
    "\n",
    "    with open(inp, \"r+\") as f:\n",
    "        el = 0\n",
    "        for line in f:\n",
    "            idx = int(line.strip())\n",
    "            add(el, idx)\n",
    "            el += 1\n",
    "\n",
    "    with open(out, \"w+\") as f:\n",
    "        idxs = list(partitions.keys())\n",
    "        idxs.sort()\n",
    "        assert(idxs[0] <= idxs[-1])\n",
    "        for ix in idxs:\n",
    "            f.write(\" \".join([str(e) for e in partitions[ix]]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "graphs/bcsstk30.graph\nCommand exited with non-zero status 124\nreal 180.01\nuser 179.75\nsys 0.22\ndecomp on graphs/bcsstk30.graph did not finish in time\ngraphs/cs4.graph\nreal 84.71\nuser 136.27\nsys 0.38\ngraphs/bcsstk30.graph.out\nterminate called after throwing an instance of 'std::length_error'\n  what():  vector::reserve\ntimeout: the monitored command dumped core\nCommand terminated by signal 6\nreal 0.25\nuser 0.00\nsys 0.00\ndecomp on graphs/bcsstk30.graph.out did not finish in time\ngraphs/vibrobox.graph\nCommand exited with non-zero status 124\nreal 180.00\nuser 179.42\nsys 0.58\ndecomp on graphs/vibrobox.graph did not finish in time\ngraphs/vibrovox.csv\ntimeout: the monitored command dumped core\nCommand terminated by signal 11\nreal 0.13\nuser 0.00\nsys 0.00\ndecomp on graphs/vibrovox.csv did not finish in time\ngraphs/crack.graph\nreal 11.11\nuser 32.15\nsys 0.15\ngraphs/fe_body.graph\na.out: main.cpp:3210: int main(int, char**): Assertion `connected(gc.g)' failed.\ntimeout: the monitored command dumped core\nCommand terminated by signal 6\nreal 0.27\nuser 0.12\nsys 0.01\ndecomp on graphs/fe_body.graph did not finish in time\ngraphs/auto.graph\nCommand exited with non-zero status 124\nreal 180.04\nuser 179.19\nsys 0.78\ndecomp on graphs/auto.graph did not finish in time\ngraphs/memplus.graph\nreal 43.86\nuser 43.52\nsys 0.28\nThe number of partitions should be greater than 1!\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'graphs/memplus.graph.part.1'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7408490215c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdecomp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cut.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmetis_partition_file_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetis_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetis_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mrw_graph\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".row_whole\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7d4e91439ca1>\u001b[0m in \u001b[0;36mmetis_partition_file_converter\u001b[0;34m(inp, out)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'graphs/memplus.graph.part.1'"
     ]
    }
   ],
   "source": [
    "#!for graph in graphs/*.graph; do echo \"time start\"; time timeout 5m ./a.out  --G_phi=0.05 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph | tee \"$graph.out\" ; echo \"time end\"; done\n",
    "#!SHELL=/bin/bash\n",
    "import os\n",
    "import glob\n",
    "\n",
    "graph_files = glob.glob(\"graphs/*\")\n",
    "graph_files.sort()\n",
    "for graph in graph_files:\n",
    "    print(graph)\n",
    "\n",
    "    #run decomposition on all graphs with time limit\n",
    "    out_path = graph + \".out\"\n",
    "    !time -p timeout 3m ./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f $graph > \"$out_path\"\n",
    "\n",
    "    #TODO return code\n",
    "    if not glob.glob(graph + \"cut.txt\"):\n",
    "        print(\"decomp on\", graph, \"did not finish in time\")\n",
    "        continue\n",
    "\n",
    "    #how many clusters, k, did we get?\n",
    "    with open(out_path) as f:\n",
    "        lns = f.readlines()\n",
    "        cluster_line = next(l for l in lns if \"n clusters\" in l)\n",
    "        cluster_line.strip(\"\\n\")\n",
    "        n_clusters = int(cluster_line.split(\";\")[1])\n",
    "\n",
    "    if n_clusters == 1:\n",
    "        print(\"No cut found, continue\")\n",
    "        continue\n",
    "\n",
    "    #run metis on k\n",
    "    metis_stdio_path = graph + \".out.metis\"\n",
    "    !/usr/local/bin/gpmetis -ufactor=1000 $graph $n_clusters -contig >  $metis_stdio_path\n",
    "\n",
    "    metis_file = graph + \".part.\" + str(n_clusters)\n",
    "    decomp_file = graph + \"cut.txt\"\n",
    "    \n",
    "    metis_partition_file_converter(metis_file, metis_file)\n",
    "\n",
    "    rw_graph      = graph + \".row_whole\"\n",
    "    rw_file_ours  = graph + \".rw_ours\"\n",
    "    rw_file_metis = graph + \".rw_metis\"\n",
    "\n",
    "    !./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander  > \"$rw_graph\"\n",
    "    !./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander -p \"$decomp_file\" > \"$rw_file_ours\"\n",
    "    !./a.out  --G_phi=0.025 --H_phi=0.4 --vol=1 --h_ratio=0. -f \"$graph\" --only_test_expander -p \"$metis_file\"  > \"$rw_file_metis\"\n",
    " \n",
    "    bname = os.path.basename(graph).split(\".\")[0]\n",
    "    !mkdir -p results/\"$bname\"\n",
    "    for f in glob.glob(\"\".join(graph.split(\".\")[:-1]) + \"*\"):\n",
    "        if f != graph:\n",
    "            !mv $f results/\"$bname\"/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "n = 3\n",
    "matrix = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\n",
    "matrix = np.array(matrix)\n",
    "\n",
    "uniform = [ 1/n for _ in range(n)]\n",
    "uniform = np.array(uniform)\n",
    "\n",
    "walk    = [1] + [0 for _ in range(n - 1)]\n",
    "walk    = np.array(walk)\n",
    "\n",
    "matrix = normalize(matrix, axis=1, norm='l1')\n",
    "\n",
    "\n",
    "distance = pairwise_distances((matrix @ walk).reshape(-1, 1), uniform.reshape(-1, 1))\n",
    "distance = pairwise_distances((matrix @ (matrix @ (matrix @ walk))).reshape(-1, 1), uniform.reshape(-1, 1))\n",
    "steps = 0\n",
    "threshold = 0.01\n",
    "while np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) > threshold:\n",
    "    walk = matrix @ walk\n",
    "    print(walk)\n",
    "    print( np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))))\n",
    "    steps += 1\n",
    "\n",
    "print(\"steps\", steps)\n",
    "\n",
    "\n",
    "def test_convergence(matrix, threshold):\n",
    "    np.set_printoptions(threshold=np.inf)    \n",
    "    n = len(matrix)\n",
    "    matrix = np.array(matrix)\n",
    "    uniform = np.array([1./n for _ in range(n)])\n",
    "    walk = np.array([1] + [0 for _ in range(n - 1)]) #Seed?\n",
    "\n",
    "    matrix = np.array([[e if i !=j else np.count_nonzero(subl == 1) for i, e in enumerate(subl)] for j, subl in enumerate(matrix)])\n",
    "    matrix = normalize(matrix, axis=0, norm='l1')\n",
    "    print(matrix)\n",
    "    steps = 0\n",
    "    dist = np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) / n\n",
    "    while dist > threshold:\n",
    "        walk = matrix @ walk\n",
    "        steps += 1\n",
    "        dist = np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) / n\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "            print(walk)\n",
    "            #print(dist)\n",
    "        if steps > 500:\n",
    "            break\n",
    "        print(steps)\n",
    "    return steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0, [6, 14\nn: 2395\ne 14921\nstart iterate\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def graph_and_cut_to_numpy(gf, cf):\n",
    "\n",
    "    graph = {}\n",
    "    with open(gf, \"r\") as f:\n",
    "        first_row = f.readline().strip(\"\\n\").split(\" \")\n",
    "        while [] in first_row:\n",
    "            first_row.remove([])\n",
    "        n, e = [int(e) for e in first_row]\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            graph[i] = []\n",
    "            for v in row.strip(\"\\n\").split(\" \"):\n",
    "                if v == \"\":\n",
    "                    continue\n",
    "                assert(int(v))\n",
    "                graph[i].append(int(v))\n",
    "\n",
    "    clusters = []\n",
    "    with open(cf, \"r\") as f:\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            clusters.append([int(e) for e in row.strip(\"\\n\").split(\" \") if e != \"\"])\n",
    "\n",
    "    return graph, clusters\n",
    "\n",
    "graph, cuts = graph_and_cut_to_numpy(\"graphs/add20.graph\", \"old_results/add20.graphcut.txt\")\n",
    "\n",
    "subgs = []\n",
    "for cut in cuts:\n",
    "\n",
    "    v_set = set()\n",
    "    for el in cut:\n",
    "        v_set.add(int(el))\n",
    "\n",
    "    #print(v_set)\n",
    "    n = len(v_set)\n",
    "\n",
    "    subg = {k:[] for k in cut}\n",
    "    for k in graph:\n",
    "        for v in graph[k]:\n",
    "            assert(int(v))\n",
    "            if v in v_set:\n",
    "                subg[k].append(v)\n",
    "    #print \n",
    "    print(\" \".join([str((e, subg[e])) for e in subg])[:10])\n",
    "    #rekey\n",
    "    rekey_dict = {v:i for i, v in enumerate(subg)}\n",
    "    rekey_reverse= {i:v for i, v in enumerate(subg)}\n",
    "\n",
    "    new_subg = [[0 for _ in range(n)] for _ in range(n)]\n",
    "    for k in subg: \n",
    "        for v in subg[k]:\n",
    "            new_subg[rekey_dict[k]][rekey_dict[v]] = 1\n",
    "    \n",
    "    test_convergence(new_subg, 0.01)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "from IPython.display import Javascript, display\n",
    "#display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)'))\n",
    "\n",
    "\n",
    "Javascript('IPython.notebook.execute_cells_below()')\n",
    "#button = w.Button(button_style='info',description=\"Execute `Report\")\n",
    "#button.on_click(run_all)\n",
    "#display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING - disconnected cluster 0\nWARNING - disconnected cluster 1\nWARNING - disconnected cluster 2\nWARNING - disconnected cluster 3\nWARNING - disconnected cluster 4\nWARNING - disconnected cluster 5\nWARNING - disconnected cluster 6\nWARNING - disconnected cluster 7\nWARNING - disconnected cluster 8\nWARNING - disconnected cluster 9\nWARNING - disconnected cluster 10\nWARNING - disconnected cluster 11\nWARNING - disconnected cluster 12\nWARNING - disconnected cluster 13\nWARNING - disconnected cluster 14\nWARNING - disconnected cluster 15\nWARNING - disconnected cluster 16\nWARNING - disconnected cluster 17\nWARNING - disconnected cluster 18\nWARNING - disconnected cluster 19\nWARNING - disconnected cluster 20\nWARNING - disconnected cluster 21\nWARNING - disconnected cluster 22\nWARNING - disconnected cluster 23\nWARNING - disconnected cluster 24\nWARNING - disconnected cluster 25\nWARNING - disconnected cluster 26\nWARNING - disconnected cluster 27\nWARNING - disconnected cluster 28\nn: 47\ne 289\nstart iterate\ndist 0.009385559692062841\nCluster 29 of size:  47 took n steps to converge: 46\nsteps/ log2 nodes 8.281441026207862\nWARNING - disconnected cluster 30\nWARNING - disconnected cluster 31\nWARNING - disconnected cluster 32\nWARNING - disconnected cluster 33\nWARNING - disconnected cluster 34\nWARNING - disconnected cluster 35\nWARNING - disconnected cluster 36\nWARNING - disconnected cluster 37\nWARNING - disconnected cluster 38\nWARNING - disconnected cluster 39\nWARNING - disconnected cluster 40\n"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\"\"\"\n",
    "n = 3\n",
    "matrix = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\n",
    "matrix = np.array(matrix)\n",
    "\n",
    "uniform = [ 1/n for _ in range(n)]\n",
    "uniform = np.array(uniform)\n",
    "\n",
    "walk    = [1] + [0 for _ in range(n - 1)]\n",
    "walk    = np.array(walk)\n",
    "\n",
    "matrix = normalize(matrix, axis=1, norm='l1')\n",
    "\n",
    "distance = pairwise_distances((matrix @ walk).reshape(-1, 1), uniform.reshape(-1, 1))\n",
    "distance = pairwise_distances((matrix @ (matrix @ (matrix @ walk))).reshape(-1, 1), uniform.reshape(-1, 1))\n",
    "steps = 0\n",
    "threshold = 0.01\n",
    "while np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) > threshold:\n",
    "    walk = matrix @ walk\n",
    "    print(walk)\n",
    "    print( np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))))\n",
    "    steps += 1\n",
    "\n",
    "print(\"steps\", steps)\n",
    "\"\"\"\n",
    "\n",
    "def test_convergence(matrix, threshold):\n",
    "    #np.set_printoptions(threshold=np.inf)    \n",
    "    n = len(matrix)\n",
    "    assert(len(matrix[0]) == n)\n",
    "    print(\"n:\", n)\n",
    "    #matrix = np.array(matrix)\n",
    "    e = sum([sum(row) for row in matrix])\n",
    " \n",
    "    print(\"e\", e)\n",
    "\n",
    "    colsums = [sum([matrix[i][j] for j in range(n)]) for i in range(n)]\n",
    "    uniform = np.array([colsums[i]/e for i in range(n)], dtype=np.float64)\n",
    "    #uniform = np.array([1] + [0 for _ in range(n - 1)]) #np.array([sum([e for e in matrix[i]])/e for i in range(n)])\n",
    "    assert(0.99 <=sum(uniform) <= 1.01)\n",
    "    walk = np.array([1.] + [0 for _ in range(n - 1)], np.float64) #Seed?\n",
    "    #walk = np.array([1./n for _ in range(n)])\n",
    "\n",
    "    sums = [sum(matrix[i]) for i in range(n)]\n",
    "    for s in sums:\n",
    "        pass #assert(s > 0)\n",
    "    matrix_with_drift = [[e/sums[j] for i, e in enumerate(subl)] for j, subl in enumerate(matrix)]\n",
    "\n",
    "    assert(len(matrix_with_drift) == n)\n",
    "    assert(len(matrix_with_drift[0]) == n)\n",
    "\n",
    "    for i in range(n):\n",
    "        matrix_with_drift[i][i] = sum(matrix_with_drift[i])\n",
    "        assert(matrix_with_drift[i][i] > 0)\n",
    "        cnt = sum(matrix_with_drift[i])\n",
    "        matrix_with_drift[i] = [m/cnt for m in matrix_with_drift[i]]\n",
    "        assert(0.99 <= sum(matrix_with_drift[i]) <= 1.01)\n",
    "\n",
    "    matrix = matrix_with_drift\n",
    "    matrix = np.array(matrix, np.float64)\n",
    "\n",
    "    dist = np.sum(pairwise_distances(walk.reshape(-1, 1), uniform.reshape(-1, 1))) / n\n",
    "    steps = 0\n",
    "    print(\"start iterate\")\n",
    "\n",
    "    while dist > threshold and steps < 500000:\n",
    "        walk = np.matmul(matrix.T, walk.T)\n",
    "        assert(0.99 < sum(walk) < 1.01)\n",
    "        assert(0.99 < sum(uniform) < 1.01)\n",
    "        steps += 1\n",
    "        dist = np.linalg.norm(walk - uniform, 2)\n",
    "\n",
    "    print(\"dist\", dist)\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def graph_and_cut_to_numpy(gf, cf):\n",
    "\n",
    "    graph = {}\n",
    "    with open(gf, \"r\") as f:\n",
    "        first_row = f.readline().strip(\"\\n\").split(\" \")\n",
    "        while [] in first_row:\n",
    "            first_row.remove([])\n",
    "        n, e = [int(e) for e in first_row]\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            graph[i] = []\n",
    "            for v in row.strip(\"\\n\").split(\" \"):\n",
    "                if v == \"\":\n",
    "                    continue\n",
    "                assert(int(v))\n",
    "                graph[i].append(int(v))\n",
    "                try:\n",
    "                    graph[v].append(int(i))\n",
    "                except KeyError:\n",
    "                    graph[v] = [int(i)]\n",
    "\n",
    "    clusters = []\n",
    "    with open(cf, \"r\") as f:\n",
    "        n_counter = 0\n",
    "        e_counter = 0\n",
    "        for i, row in enumerate(f.readlines()):\n",
    "            clusters.append([int(e) for e in row.strip(\"\\n\").split(\" \") if e != \"\"])\n",
    "            #print(clusters[-1])\n",
    "            n_counter += 1\n",
    "        e_counter += len(clusters[-1])\n",
    "\n",
    "\n",
    "    return graph, clusters\n",
    "\n",
    "\n",
    "def test():\n",
    "    graph, cuts = graph_and_cut_to_numpy(\"graphs/3elt.graph\", \"old_results/3elt.graphcut.txt\")\n",
    "    #graph, cuts = graph_and_cut_to_numpy(\"graphs/whitaker3.graph\", \"old_results/whitaker3.graph.part.94.chaco\")\n",
    "    subgs = []\n",
    "\n",
    "    for clidx, cut in enumerate(cuts):\n",
    "\n",
    "        v_set = set()\n",
    "        for el in cut:\n",
    "            v_set.add(int(el))\n",
    "\n",
    "        #print(v_set)\n",
    "        n = len(v_set)\n",
    "\n",
    "        subg = {k:[] for k in cut}\n",
    "        #print(\"first element in cut\", cut[0])\n",
    "        for k in graph:\n",
    "            for v in graph[k]:\n",
    "                if v in v_set and k in v_set:\n",
    "                    subg[k].append(v)\n",
    "\n",
    "        rekey_dict    = {v:i for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "        rekey_reverse = {i:v for i, v in enumerate(sorted(list(subg.keys())))}\n",
    "\n",
    "        success = True\n",
    "        new_subg = [[0 for _ in range(n)] for _ in range(n)]\n",
    "        for k in subg: \n",
    "            #assert(len(subg[k]) > 0)\n",
    "            if len(subg[k]) == 0:\n",
    "                print(\"WARNING - disconnected cluster\", clidx)\n",
    "                success = False\n",
    "                break\n",
    "            for v in subg[k]:\n",
    "                new_subg[rekey_dict[k]][rekey_dict[v]] = 1\n",
    "                new_subg[rekey_dict[v]][rekey_dict[k]] = 1\n",
    "        if not success:\n",
    "            continue\n",
    "        assert(len(new_subg) == n)\n",
    "        assert(len(new_subg[0]) == n)\n",
    "        steps = test_convergence(new_subg, 0.01)\n",
    "        print(\"Cluster\", clidx, \"of size: \", n, \"took n steps to converge:\", steps)\n",
    "        import math\n",
    "        print(\"steps/ log2 nodes\", steps/math.log2(n))\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
